<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Naive Bayes classifier for texts — textmodel_nb • quanteda.textmodels</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Naive Bayes classifier for texts — textmodel_nb" />
<meta property="og:description" content="Fit a multinomial or Bernoulli Naive Bayes model, given a dfm and some
training labels." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">quanteda.textmodels</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.9.2.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/textmodel_performance.html">textmodel Performance Comparisons</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/quanteda/quanteda.textmodels/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Naive Bayes classifier for texts</h1>
    <small class="dont-index">Source: <a href='https://github.com/quanteda/quanteda.textmodels/blob/master/R/textmodel_nb.R'><code>R/textmodel_nb.R</code></a></small>
    <div class="hidden name"><code>textmodel_nb.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Fit a multinomial or Bernoulli Naive Bayes model, given a dfm and some
training labels.</p>
    </div>

    <pre class="usage"><span class='fu'>textmodel_nb</span><span class='op'>(</span>
  <span class='va'>x</span>,
  <span class='va'>y</span>,
  smooth <span class='op'>=</span> <span class='fl'>1</span>,
  prior <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"uniform"</span>, <span class='st'>"docfreq"</span>, <span class='st'>"termfreq"</span><span class='op'>)</span>,
  distribution <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"multinomial"</span>, <span class='st'>"Bernoulli"</span><span class='op'>)</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>the <a href='https://quanteda.io/reference/dfm.html'>dfm</a> on which the model will be fit.  Does not need to
contain only the training documents.</p></td>
    </tr>
    <tr>
      <th>y</th>
      <td><p>vector of training labels associated with each document identified
in <code>train</code>.  (These will be converted to factors if not already
factors.)</p></td>
    </tr>
    <tr>
      <th>smooth</th>
      <td><p>smoothing parameter for feature counts, added to the
feature frequency totals by training class</p></td>
    </tr>
    <tr>
      <th>prior</th>
      <td><p>prior distribution on texts; one of <code>"uniform"</code>,
<code>"docfreq"</code>, or <code>"termfreq"</code>.  See Prior Distributions below.</p></td>
    </tr>
    <tr>
      <th>distribution</th>
      <td><p>count model for text features, can be <code>multinomial</code> or
<code>Bernoulli</code>.  To fit a "binary multinomial" model, first convert the dfm to
a binary matrix using <code>[quanteda::dfm_weight](x, scheme = "boolean")</code>.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p><code>textmodel_nb()</code> returns a list consisting of the following (where
\(I\) is the total number of documents, \(J\) is the total number of
features, and \(k\) is the total number of training classes):</p>
<dt>call</dt><dd><p>original function call</p></dd>

<dt>param</dt><dd><p>\(k \times V\); class conditional posterior estimates</p></dd>

<dt>x</dt><dd><p>the \(N \times V\) training dfm <code>x</code></p></dd>

<dt>y</dt><dd><p>the \(N\)-length <code>y</code> training class vector, where NAs will
not be used will be retained in the saved <code>x</code> matrix</p></dd>

<dt>distribution</dt><dd><p>character; the distribution of <code>x</code> for the NB
model</p></dd>

<dt>priors</dt><dd><p>numeric; the class prior probabilities</p></dd>

<dt>smooth</dt><dd><p>numeric; the value of the smoothing parameter</p></dd>

    <h2 class="hasAnchor" id="prior-distributions"><a class="anchor" href="#prior-distributions"></a>Prior distributions</h2>

    


<p>Prior distributions refer to the prior probabilities assigned to the
training classes, and the choice of prior distribution affects the
calculation of the fitted probabilities.  The default is uniform priors,
which sets the unconditional probability of observing the one class to be
the same as observing any other class.</p>
<p>"Document frequency" means that the class priors will be taken from the
relative proportions of the class documents used in the training set.  This
approach is so common that it is assumed in many examples, such as the
worked example from Manning, Raghavan, and Schütze (2008) below.  It is not
the default in <span class="pkg">quanteda</span>, however, since there may be nothing
informative in the relative numbers of documents used to train a classifier
other than the relative availability of the documents.  When training
classes are balanced in their number of documents (usually advisable),
however, then the empirically computed "docfreq" would be equivalent to
"uniform" priors.</p>
<p>Setting <code>prior</code> to "termfreq" makes the priors equal to the proportions of
total feature counts found in the grouped documents in each training class,
so that the classes with the largest number of features are assigned the
largest priors. If the total count of features in each training class was
the same, then "uniform" and "termfreq" would be the same.</p>
    <h2 class="hasAnchor" id="smoothing-parameter"><a class="anchor" href="#smoothing-parameter"></a>Smoothing parameter</h2>

    


<p>The <code>smooth</code> value is added to the feature frequencies, aggregated by
training class, to avoid zero frequencies in any class.  This has the
effect of giving more weight to infrequent term occurrences.</p>
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Manning, C.D., Raghavan, P., &amp; Schütze, H. (2008). <em>An
Introduction to Information Retrieval</em>. Cambridge: Cambridge University
Press (Chapter 13). Available at
<a href='https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf'>https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>.</p>
<p>Jurafsky, D. &amp; Martin, J.H. (2018). From <em>Speech and Language Processing:
An Introduction to Natural Language Processing, Computational Linguistics,
and Speech Recognition</em>. Draft of September 23, 2018 (Chapter 6, Naive
Bayes). Available at <a href='https://web.stanford.edu/~jurafsky/slp3/'>https://web.stanford.edu/~jurafsky/slp3/</a>.</p>
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><code><a href='predict.textmodel_nb.html'>predict.textmodel_nb()</a></code></p></div>
    <h2 class="hasAnchor" id="author"><a class="anchor" href="#author"></a>Author</h2>

    <p>Kenneth Benoit</p>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'>## Example from 13.1 of _An Introduction to Information Retrieval_</span>
<span class='va'>txt</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span>d1 <span class='op'>=</span> <span class='st'>"Chinese Beijing Chinese"</span>,
         d2 <span class='op'>=</span> <span class='st'>"Chinese Chinese Shanghai"</span>,
         d3 <span class='op'>=</span> <span class='st'>"Chinese Macao"</span>,
         d4 <span class='op'>=</span> <span class='st'>"Tokyo Japan Chinese"</span>,
         d5 <span class='op'>=</span> <span class='st'>"Chinese Chinese Chinese Tokyo Japan"</span><span class='op'>)</span>
<span class='va'>x</span> <span class='op'>&lt;-</span> <span class='fu'>quanteda</span><span class='fu'>::</span><span class='fu'><a href='https://quanteda.io/reference/dfm.html'>dfm</a></span><span class='op'>(</span><span class='va'>txt</span>, tolower <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>
<span class='va'>y</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/factor.html'>factor</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"N"</span>, <span class='cn'>NA</span><span class='op'>)</span>, ordered <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>

<span class='co'>## replicate IIR p261 prediction for test set (document 5)</span>
<span class='op'>(</span><span class='va'>tmod1</span> <span class='op'>&lt;-</span> <span class='fu'>textmodel_nb</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span>, prior <span class='op'>=</span> <span class='st'>"docfreq"</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; 
#&gt; Call:
#&gt; textmodel_nb.dfm(x = x, y = y, prior = "docfreq")
#&gt; 
#&gt;  Distribution: multinomial ; priors: 0.25 0.75 ; smoothing value: 1 ; 4 training documents;  fitted features. </div><div class='input'><span class='fu'><a href='https://rdrr.io/r/base/summary.html'>summary</a></span><span class='op'>(</span><span class='va'>tmod1</span><span class='op'>)</span>
</div><div class='output co'>#&gt; 
#&gt; Call:
#&gt; textmodel_nb.dfm(x = x, y = y, prior = "docfreq")
#&gt; 
#&gt; Class Priors:
#&gt; (showing first 2 elements)
#&gt;    N    Y 
#&gt; 0.25 0.75 
#&gt; 
#&gt; Estimated Feature Scores:
#&gt;   Chinese Beijing Shanghai  Macao   Tokyo   Japan
#&gt; N  0.2222  0.1111   0.1111 0.1111 0.22222 0.22222
#&gt; Y  0.4286  0.1429   0.1429 0.1429 0.07143 0.07143</div><div class='input'><span class='fu'><a href='https://rdrr.io/r/stats/coef.html'>coef</a></span><span class='op'>(</span><span class='va'>tmod1</span><span class='op'>)</span>
</div><div class='output co'>#&gt;                  N          Y
#&gt; Chinese  0.2222222 0.42857143
#&gt; Beijing  0.1111111 0.14285714
#&gt; Shanghai 0.1111111 0.14285714
#&gt; Macao    0.1111111 0.14285714
#&gt; Tokyo    0.2222222 0.07142857
#&gt; Japan    0.2222222 0.07142857</div><div class='input'><span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='va'>tmod1</span>, type <span class='op'>=</span> <span class='st'>"prob"</span><span class='op'>)</span>
</div><div class='output co'>#&gt;             N         Y
#&gt; d1 0.06516267 0.9348373
#&gt; d2 0.06516267 0.9348373
#&gt; d3 0.11850060 0.8814994
#&gt; d4 0.62587672 0.3741233
#&gt; d5 0.31024139 0.6897586</div><div class='input'><span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='va'>tmod1</span><span class='op'>)</span>
</div><div class='output co'>#&gt; d1 d2 d3 d4 d5 
#&gt;  Y  Y  Y  N  Y 
#&gt; Levels: N Y</div><div class='input'>
<span class='co'># contrast with other priors</span>
<span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='fu'>textmodel_nb</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span>, prior <span class='op'>=</span> <span class='st'>"uniform"</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; d1 d2 d3 d4 d5 
#&gt;  Y  Y  Y  N  N 
#&gt; Levels: N Y</div><div class='input'><span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='fu'>textmodel_nb</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span>, prior <span class='op'>=</span> <span class='st'>"termfreq"</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; d1 d2 d3 d4 d5 
#&gt;  Y  Y  Y  N  Y 
#&gt; Levels: N Y</div><div class='input'>
<span class='co'>## replicate IIR p264 Bernoulli Naive Bayes</span>
<span class='va'>tmod2</span> <span class='op'>&lt;-</span> <span class='fu'>textmodel_nb</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span>, distribution <span class='op'>=</span> <span class='st'>"Bernoulli"</span>, prior <span class='op'>=</span> <span class='st'>"docfreq"</span><span class='op'>)</span>
<span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='va'>tmod2</span>, newdata <span class='op'>=</span> <span class='va'>x</span><span class='op'>[</span><span class='fl'>5</span>, <span class='op'>]</span>, type <span class='op'>=</span> <span class='st'>"prob"</span><span class='op'>)</span>
</div><div class='output co'>#&gt;            N         Y
#&gt; d5 0.8089332 0.1910668</div><div class='input'><span class='fu'><a href='https://rdrr.io/r/stats/predict.html'>predict</a></span><span class='op'>(</span><span class='va'>tmod2</span>, newdata <span class='op'>=</span> <span class='va'>x</span><span class='op'>[</span><span class='fl'>5</span>, <span class='op'>]</span><span class='op'>)</span>
</div><div class='output co'>#&gt; d5 
#&gt;  N 
#&gt; Levels: N Y</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Kenneth Benoit, Kohei Watanabe, Haiyan Wang, Patrick O. Perry, Benjamin Lauderdale, Johannes Gruber, William Lowe, European Research Council.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


